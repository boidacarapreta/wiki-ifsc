#cloud-config
hostname: coreos-0
ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDm6baOg9AXEsNZCDwmIr32SEwfhoVvmT4WrsfMtJlPD5zfpGydC0oNbBtmB7DbwANSs79hCgsQw2SGgOumUQMPTCjdrK3DgpUuHZHnLbf/LdOvaM34HzVEwSJ8E24H3272ypdW9LxpNfDp3lwVv9UHQqc1pZ69nQkkEOq7OMozQD8inWDLhMfdXfkGtr8LwYV+Cq9H0jy+V5yAYkC5wMwf/uIVTwfocPZ4wn0iQTj6l/JVInb6A1RsxfhSbHtU+AXYxVa5zi/KJFSExkaIiuKYdTTN6/4jRJuwld7SdJ+f4c8Vh481WqisJCF339lGcVbfoTGdk7JwRNj/rWJrzBCZelVJxohB9eMC5yG6HFWQTb3CwboR6trQ0BK/pcwQMx90UlA8LEdNrGziO8djZIYvT4SWoZEwTe7Wq6s++YNkjs0Jjca+nK0xbci10zgApDUhWhArBdF0hIjVK22Q3g0HizpiwzTohcQfvMyIgrUmXXPAIbFF6Zit57PxzNQn9zHjz+eyznlIh119TnnbRnY7rYL9BT1sEqmb4lpl/H5H4iSfvVLsXrhWu3tXAkBFV1mUcSznfnjUJvLLPVY/p8dc5k4Ux57pOhuhwaGUC2UVjPGWRZw3hvcnqVESOPw8PduP/RWRz6xRH5BgO6YKRvvRhMtTp5+K7CSxgpBuCUcN3Q== boidacarapreta@gmail.com
write_files:
  - path: /etc/modprobe.d/bonding.conf
    permissions: 0644
    owner: root
    content: |
      options bonding max_bonds=0
  - path: /etc/systemd/network/10-en.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=en*
      [Network]
      Bond=bond0
  - path: /etc/systemd/network/20-bond0.netdev
    permissions: 0644
    owner: root
    content: |
      [NetDev]
      Name=bond0
      Kind=bond
      [Bond]
      Mode=802.3ad
      MIIMonitorSec=1
      LACPTransmitRate=fast
      TransmitHashPolicy=layer2+3
  - path: /etc/systemd/network/21-bond0.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=bond0
      [Network]
      VLAN=vlan110
      VLAN=vlan111
      VLAN=vlan900
  - path: /etc/systemd/network/30-vlan110.netdev
    permissions: 0644
    owner: root
    content: |
      [NetDev]
      Name=vlan110
      Kind=vlan
      [VLAN]
      Id=110
  - path: /etc/systemd/network/31-vlan111.netdev
    permissions: 0644
    owner: root
    content: |
      [NetDev]
      Name=vlan111
      Kind=vlan
      [VLAN]
      Id=111
  - path: /etc/systemd/network/32-vlan900.netdev
    permissions: 0644
    owner: root
    content: |
      [NetDev]
      Name=vlan900
      Kind=vlan
      [VLAN]
      Id=900
  - path: /etc/systemd/network/40-vlan110-ip.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=vlan110
      [Network]
      DNS=200.135.37.72
      DNS=200.135.37.65
      Address=172.18.110.100/24
      Address=FC00:110::100/64
      Gateway=172.18.110.254
  - path: /etc/systemd/network/41-vlan111-ip.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=vlan111
      [Network]
      Address=172.18.111.100/24
      Address=FC00:111::100/64
  - path: /etc/systemd/network/42-vlan900-ip.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=vlan900
      [Network]
      Address=10.90.0.100/24
      Address=FC00:900::100/64
  - path: /etc/systemd/timesyncd.conf
    permissions: 0644
    owner: root
    content: |
      [Time]
      NTP=pool.ntp.br ntp.ufsc.br ntp.cais.rnp.br
  - path: /etc/flannel/options.env
    permissions: 0644
    owner: root
    content: |
      FLANNELD_IFACE=172.18.111.100
      FLANNELD_ETCD_ENDPOINTS=http://172.18.111.100:2379,http://172.18.111.101:2379
  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    permissions: 0644
    owner: root
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:v1.2.3_coreos.cni.1
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=http://172.18.111.100:2380,http://172.18.111.101:2380
          - --allow-privileged=true
          - --service-cluster-ip-range=10.1.0.0/16
          - --secure-port=443
          - --advertise-address=172.18.111.100
          - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/thirdpartyresources=true
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    permissions: 0644
    owner: root
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:v1.2.3_coreos.cni.1
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --proxy-mode=iptables
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    permissions: 0644
    owner: root
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:v1.2.3_coreos.cni.1
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 1
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path:  /etc/kubernetes/manifests/kube-scheduler.yaml
    permissions: 0644
    owner: root
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:v1.2.3_coreos.cni.1
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 1
  - path: /etc/kubernetes/manifests/policy-agent.yaml
    permissions: 0644
    owner: root
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: calico-policy-agent
        namespace: calico-system
      spec:
        hostNetwork: true
        containers:
          # The Calico policy agent.
          - name: k8s-policy-agent
            image: calico/k8s-policy-agent:v0.1.4
            env:
              - name: ETCD_ENDPOINTS
                value: "http://172.18.111.100:2379,http://172.18.111.101:2379"
              - name: K8S_API
                value: "http://127.0.0.1:8080"
              - name: LEADER_ELECTION
                value: "true"
          # Leader election container used by the policy agent.
          - name: leader-elector
            image: quay.io/calico/leader-elector:v0.1.0
            imagePullPolicy: IfNotPresent
            args:
              - "--election=calico-policy-election"
              - "--election-namespace=calico-system"
              - "--http=127.0.0.1:4040"
  - path: /etc/kubernetes/cni/net.d/10-calico.conf
    permissions: 0644
    owner: root
    content: |
      {
          "name": "calico",
          "type": "flannel",
          "delegate": {
              "type": "calico",
              "etcd_endpoints": "http://172.18.111.100:2379,http://172.18.111.101:2379",
              "log_level": "none",
              "log_level_stderr": "info",
              "hostname": "172.18.111.100",
              "policy": {
                  "type": "k8s",
                  "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
              }
          }
      }
coreos:
  units:
    - name: systemd-networkd.service
      command: restart
      enable: true
    - name: router.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=router+NAT
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStart=/usr/sbin/sysctl -w net.ipv4.conf.all.forwarding=1
        ExecStart=/usr/sbin/iptables -w -t nat -F -w
        ExecStart=/usr/sbin/iptables -w -t nat -A POSTROUTING -s 172.18.111.0/24 -o vlan110 -j MASQUERADE
        Type=oneshot
    - name: settimezone.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Set the time zone
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone America/Sao_Paulo
        RemainAfterExit=yes
        Type=oneshot
    - name: etcd2.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=etcd2
        Conflicts=etcd.service
        Requires=network-online.target
        After=network-online.target
        [Service]
        User=etcd
        Type=notify
        Environment=ETCD_DATA_DIR=/var/lib/etcd2
        Environment=ETCD_NAME=%m
        ExecStart=/usr/bin/etcd2
        Restart=always
        RestartSec=10s
        LimitNOFILE=40000
        TimeoutStartSec=0
        Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.18.111.100:2379"
        Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.18.111.100:2380"
        Environment="ETCD_INITIAL_CLUSTER=coreos-0=http://172.18.111.100:2380"
        Environment="ETCD_INITIAL_CLUSTER_STATE=new"
        Environment="ETCD_LISTEN_CLIENT_URLS=http://172.18.111.100:2379,http://127.0.0.1:2379"
        Environment="ETCD_LISTEN_PEER_URLS=http://172.18.111.100:2380"
        Environment="ETCD_NAME=coreos-0"
    - name: flanneld.service
      command: start
      enable: true
      drop-ins:
      - name: 40-ExecStartPre-symlink.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      - name: 50-network-config.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network":"10.0.0.0/16","Backend":{"Type":"vxlan"}}'
    - name: docker.service
      command: start
      enable: true
      drop-ins:
      - name: 40-flannel.conf
        content: |
          [Unit]
          Wants=flanneld.service
          After=flanneld.service
          [Service]
          ExecStartPre=/usr/bin/pidof flanneld
          Restart=on-failure
          RestartSec=5s
    - name: kubelet.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/coreos/kubernetes
        Documentation=https://quay.io/repository/coreos/hyperkube?tab=tags
        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/cni/net.d
        # https://quay.io/repository/coreos/hyperkube?tag=latest&tab=tags
        Environment=KUBELET_VERSION=v1.2.3_coreos.cni.1
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --container-runtime=rkt \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=/usr/lib64/rkt/stage1-images/stage1-fly.aci \
        --api-servers=http://127.0.0.1:8080 \
        --network-plugin-dir=/etc/kubernetes/cni/net.d \
        --network-plugin=cni \
        --register-schedulable=false \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=172.18.111.100 \
        --cluster-dns=10.1.0.2 \
        --cluster-domain=ifsc-sj.local
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
    - name: calico-node.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target
        After=network-online.target
        [Service]
        Slice=machine.slice
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=172.18.111.100
        Environment=IP=172.18.111.100
        Environment=FELIX_FELIXHOSTNAME=172.18.111.100
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_ENDPOINTS=http://172.18.111.100:2379,http://172.18.111.101:2379
        ExecStart=/usr/bin/rkt run --inherit-env --stage1-from-dir=stage1-fly.aci \
        --volume=modules,kind=host,source=/lib/modules,readOnly=false \
        --mount=volume=modules,target=/lib/modules \
        --trust-keys-from-https quay.io/calico/node:v0.19.0
        KillMode=mixed
        Restart=always
        TimeoutStartSec=0
        [Install]
        WantedBy=multi-user.target
